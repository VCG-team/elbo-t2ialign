{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to generate and visualize masks for a given image\n",
    "# input: image path, class names, threshold, text_prompt, text_template, timesteps\n",
    "# output: source img, loss, cross att, heatmap, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/zq/envs/ddsseg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# package and helper functions\n",
    "import warnings\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from utils.attention_control import (\n",
    "    AttnProcessor,\n",
    ")\n",
    "from utils.diffusion import Diffusion\n",
    "from utils.img2text import Img2Text\n",
    "from utils.parse_args import parse_args\n",
    "\n",
    "T = torch.Tensor\n",
    "TL = List[T]\n",
    "TN = Optional[T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Taking `'Attention' object has no attribute 'key'` while using `accelerate.load_checkpoint_and_dispatch` to mean models/models--runwayml--stable-diffusion-v1-5/snapshots/f03de327dd89b501a01da37fc5240cf4fdba85a1/vae was saved with deprecated attention block weight names. We will load it with the deprecated attention block names and convert them on the fly to the new attention block format. Please re-save the model after this conversion, so we don't have to do the on the fly renaming in the future. If the model is from a hub checkpoint, please also re-upload it or open a PR on the original repository.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:04<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# load config and model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "config = parse_args(\"segmentation\", False, [])\n",
    "img2text = Img2Text(config)\n",
    "diffusion_dtype = torch.float16 if config.diffusion.dtype == \"fp16\" else torch.float32\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    config.diffusion.variant,\n",
    "    torch_dtype=diffusion_dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=config.model_dir,\n",
    "    device_map=config.diffusion.device_map,\n",
    ")\n",
    "# register attention processor for attention hooks\n",
    "pipe.unet.set_attn_processor(AttnProcessor())\n",
    "diffusion = Diffusion(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4309)\n",
    "np.random.seed(4309)\n",
    "torch.manual_seed(4309)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    def generate_prompt(classes, weight):\n",
    "        prompt = \"\"\n",
    "        for i, c in enumerate(classes):\n",
    "            prompt += f\"({c}){weight[i]}\"\n",
    "            if i != len(classes) - 1:\n",
    "                prompt += \", \"\n",
    "        return prompt\n",
    "\n",
    "    classes = [\n",
    "        \"a bookshelf stacked with books\",\n",
    "        \"an golden cat\",\n",
    "        \"sofa\",\n",
    "        \"a potted plant in the corner\",\n",
    "        \"a artistic landscape photo hanging on the wall\"\n",
    "    ]\n",
    "    class_weight = [1.0] * len(classes)\n",
    "    class_emb = []\n",
    "    for c in classes:\n",
    "        class_emb.append(diffusion.encode_prompt(f\"{c}\"))\n",
    "\n",
    "    # 1. prepare latent\n",
    "    channel = diffusion.unet.config.in_channels\n",
    "    height = diffusion.unet.config.sample_size\n",
    "    width = diffusion.unet.config.sample_size\n",
    "    latent = torch.randn(\n",
    "        1,\n",
    "        channel,\n",
    "        height,\n",
    "        width,\n",
    "        device=diffusion.unet.device,\n",
    "        dtype=diffusion.unet.dtype,\n",
    "    )\n",
    "\n",
    "    # 2. prepare text embedding\n",
    "    negtive_prompt = \"\"\n",
    "    neg_text_emb = diffusion.encode_prompt(negtive_prompt)\n",
    "\n",
    "    # 3. prepare timesteps\n",
    "    train_timesteps = diffusion.scheduler.config.num_train_timesteps\n",
    "    step_ratio = train_timesteps // 500\n",
    "    timesteps = list(range(train_timesteps - 1, 0, -step_ratio))\n",
    "\n",
    "    # 4. reverse diffusion process\n",
    "    for t in timesteps:\n",
    "        # 4.1. generate unweight prompt\n",
    "        prompt = generate_prompt(classes, [1] * len(classes))\n",
    "        pos_text_emb = diffusion.encode_prompt(prompt)\n",
    "        # 4.2. get unweight eps prediction\n",
    "        eps_pred_cond = diffusion.get_eps_prediction([latent], [t], [pos_text_emb])\n",
    "        # 4.3. get unweight elbo and compute alignment score\n",
    "        elbo = []\n",
    "        for c in classes:\n",
    "            eps_pred_c = diffusion.get_eps_prediction(\n",
    "                [latent], [t], [class_emb[classes.index(c)]]\n",
    "            )\n",
    "            elbo.append(F.mse_loss(eps_pred_cond, eps_pred_c, reduction=\"mean\"))\n",
    "        elbo = torch.stack(elbo)\n",
    "        elbo = (elbo - elbo.min()) / (elbo.max() - elbo.min())\n",
    "        class_weight = torch.round(torch.pow(1, elbo), decimals=2).tolist()\n",
    "        # 4.4. generate weight prompt and get weight eps prediction\n",
    "        prompt = generate_prompt(classes, class_weight)\n",
    "        pos_text_emb = diffusion.encode_prompt(prompt)\n",
    "        eps_pred_cond, eps_pred_uncond = diffusion.get_eps_prediction(\n",
    "            [latent, latent], [t, t], [pos_text_emb, neg_text_emb]\n",
    "        ).chunk(2)\n",
    "        eps_pred = diffusion.classifier_free_guidance(eps_pred_uncond, eps_pred_cond)\n",
    "        latent = diffusion.step(latent, t, max(0, t - step_ratio), eps_pred)\n",
    "\n",
    "    # 5. decode latent to image\n",
    "    img = diffusion.decode_latent(latent)[0]\n",
    "    img.save(\"./tmp/example.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
