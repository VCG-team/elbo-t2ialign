{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to generate image refined by ELBO\n",
    "# input: random seed, timesteps, ELBO strength, text prompt\n",
    "# output: generated image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package and helper functions\n",
    "import warnings\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import AutoPipelineForText2Image, StableDiffusion3Pipeline\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from utils.attention_control import AttnProcessor, JointAttnProcessor\n",
    "from utils.diffusion import Diffusion\n",
    "from utils.img2text import Img2Text\n",
    "from utils.parse_args import parse_args\n",
    "\n",
    "T = torch.Tensor\n",
    "TL = List[T]\n",
    "TN = Optional[T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config and model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "config = parse_args(\"segmentation\", False, [\"diffusion.variant=stable-diffusion-v1-5/stable-diffusion-v1-5\"])\n",
    "img2text = Img2Text(config)\n",
    "diffusion_dtype = torch.float16 if config.diffusion.dtype == \"fp16\" else torch.float32\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    config.diffusion.variant,\n",
    "    torch_dtype=diffusion_dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=config.model_dir,\n",
    "    device_map=config.diffusion.device_map,\n",
    ")\n",
    "# register attention processor for attention hooks\n",
    "if isinstance(pipe, StableDiffusion3Pipeline):\n",
    "    pipe.transformer.set_attn_processor(JointAttnProcessor())\n",
    "else:\n",
    "    pipe.unet.set_attn_processor(AttnProcessor())\n",
    "diffusion = Diffusion(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full prompt = concatenated classes\n",
    "classes = [\n",
    "    \"a bookshelf stacked with books\",\n",
    "    \"an golden cat\",\n",
    "    \"sofa\",\n",
    "    \"a potted plant in the corner\",\n",
    "    \"a artistic landscape photo hanging on the wall\",\n",
    "]\n",
    "infer_timesteps = 500\n",
    "random_seed = 4309\n",
    "elbo_strength = 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    def generate_prompt(classes, weight):\n",
    "        prompt = \"\"\n",
    "        for i, c in enumerate(classes):\n",
    "            prompt += f\"({c}){weight[i]}\"\n",
    "            if i != len(classes) - 1:\n",
    "                prompt += \", \"\n",
    "        return prompt\n",
    "\n",
    "    class_weight = [1.0] * len(classes)\n",
    "    class_emb = []\n",
    "    for c in classes:\n",
    "        class_emb.append(diffusion.encode_prompt(f\"{c}\"))\n",
    "\n",
    "    # 1. prepare latent\n",
    "    latent = diffusion.prepare_latent()\n",
    "\n",
    "    # 2. prepare text embedding\n",
    "    negtive_prompt = \"\"\n",
    "    neg_text_emb = diffusion.encode_prompt(negtive_prompt)\n",
    "\n",
    "    # 3. prepare timesteps\n",
    "    train_timesteps = diffusion.scheduler.config.num_train_timesteps\n",
    "    step_ratio = train_timesteps // infer_timesteps\n",
    "    timesteps = list(range(train_timesteps - 1, 0, -step_ratio))\n",
    "\n",
    "    # 4. reverse diffusion process\n",
    "    for t in timesteps:\n",
    "        # 4.1. generate unweight prompt\n",
    "        prompt = generate_prompt(classes, [1] * len(classes))\n",
    "        pos_text_emb = diffusion.encode_prompt(prompt)\n",
    "        # 4.2. get unweight model prediction for whole sentence\n",
    "        model_pred_all = diffusion.get_model_prediction([latent], [t], [pos_text_emb])\n",
    "        # 4.3. get unweight elbo and compute alignment score\n",
    "        elbo = []\n",
    "        for c in classes:\n",
    "            model_pred_c = diffusion.get_model_prediction(\n",
    "                [latent], [t], [class_emb[classes.index(c)]]\n",
    "            )\n",
    "            elbo.append(F.mse_loss(model_pred_all, model_pred_c, reduction=\"mean\"))\n",
    "        elbo = torch.stack(elbo)\n",
    "        elbo = (elbo - elbo.min()) / (elbo.max() - elbo.min())\n",
    "        class_weight = torch.round(torch.pow(elbo_strength, elbo), decimals=2).tolist()\n",
    "        # 4.4. generate weight prompt and get weight model prediction\n",
    "        prompt = generate_prompt(classes, class_weight)\n",
    "        pos_text_emb = diffusion.encode_prompt(prompt)\n",
    "        model_pred_cond, model_pred_uncond = diffusion.get_model_prediction(\n",
    "            [latent, latent], [t, t], [pos_text_emb, neg_text_emb]\n",
    "        ).chunk(2)\n",
    "        model_pred = diffusion.classifier_free_guidance(\n",
    "            model_pred_uncond, model_pred_cond\n",
    "        )\n",
    "        latent = diffusion.step(latent, t, max(0, t - step_ratio), model_pred)\n",
    "\n",
    "    # 5. decode latent to image\n",
    "    img = diffusion.decode_latent(latent)[0]\n",
    "    img.save(\"./tmp/example.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
