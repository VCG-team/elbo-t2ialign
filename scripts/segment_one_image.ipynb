{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to generate and visualize masks for a given image\n",
    "# input: image path, class names, threshold, text_prompt, text_template, timesteps\n",
    "# output: source img, loss, cross att, heatmap, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package and helper functions\n",
    "import random\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers.models.attention_processor import Attention\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.attention_control import (\n",
    "    AttentionStoreHook,\n",
    "    AttnProcessor,\n",
    "    aggregate_cross_att,\n",
    "    aggregate_self_att,\n",
    ")\n",
    "from utils.diffusion import Diffusion\n",
    "from utils.img2text import Img2Text\n",
    "from utils.parse_args import parse_args\n",
    "\n",
    "T = torch.Tensor\n",
    "TL = List[T]\n",
    "TN = Optional[T]\n",
    "\n",
    "\n",
    "class AvgAttentionStoreHook(AttentionStoreHook):\n",
    "    @torch.inference_mode\n",
    "    def forward(self, attn: Attention, q: T, k: T, v: T, sim: T, out: T) -> T:\n",
    "        \"\"\"\n",
    "        q: (b, h, i, d)\n",
    "        k, v: (b, h, j, d)\n",
    "        sim: (b, h, i, j)\n",
    "        out: (b, i, n)\n",
    "        \"\"\"\n",
    "        key = \"cross_att\" if attn.is_cross_attention else \"self_att\"\n",
    "        self.step_store[key].append(sim.mean(dim=1))\n",
    "        return out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassData:\n",
    "    loss_img: Optional[np.ndarray] = None\n",
    "    cross_att_img: Optional[np.ndarray] = None\n",
    "    heatmap: Optional[np.ndarray] = None\n",
    "    mask: Optional[np.ndarray] = None\n",
    "    text: str = \"\"\n",
    "    loss: TN = None\n",
    "    cross_att: TN = None\n",
    "    self_att: TN = None\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        setattr(self, key, value)\n",
    "\n",
    "\n",
    "def parse_timesteps(timesteps: List[List]) -> List:\n",
    "    parsed_timesteps = []\n",
    "    for t in timesteps:\n",
    "        if len(t) == 3:\n",
    "            parsed_timesteps.extend(range(t[0], t[1], t[2]))\n",
    "        else:  # len(t) == 4, random sample\n",
    "            parsed_timesteps.extend([random.randint(t[1], t[2]) for _ in range(t[3])])\n",
    "    return parsed_timesteps\n",
    "\n",
    "\n",
    "def show_data(img: Image, cls_to_data: Dict[str, ClassData]):\n",
    "    for cls in cls_to_data:\n",
    "        print(\n",
    "            f\"class: {cls} --- loss: {cls_to_data[cls].loss} --- text: {cls_to_data[cls].text}\"\n",
    "        )\n",
    "\n",
    "    row_cnt = len(cls_to_data)\n",
    "    col_cnt = 5\n",
    "    fig, axs = plt.subplots(row_cnt, col_cnt, figsize=(col_cnt * 3, row_cnt * 3))\n",
    "    placeholder = np.zeros((img.size[1], img.size[0]), dtype=np.uint8)\n",
    "    placeholder_title = \"No Image Available\"\n",
    "\n",
    "    if row_cnt == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, (cls, data) in enumerate(cls_to_data.items()):\n",
    "        axs[i][0].set_ylabel(cls)\n",
    "        axs[i][0].imshow(img)\n",
    "        images = [data.loss_img, data.cross_att_img, data.heatmap, data.mask]\n",
    "        for j, image in enumerate(images):\n",
    "            if image is not None:\n",
    "                axs[i][j + 1].imshow(image)\n",
    "            else:\n",
    "                axs[i][j + 1].imshow(placeholder)\n",
    "                axs[i][j + 1].set_title(placeholder_title)\n",
    "\n",
    "    for i in range(row_cnt):\n",
    "        for j in range(col_cnt):\n",
    "            axs[i][j].tick_params(\n",
    "                left=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False,\n",
    "                bottom=False,\n",
    "            )\n",
    "\n",
    "    for j, title in enumerate(\n",
    "        [\"source img\", \"loss img\", \"cross att img\", \"heatmap\", \"mask\"]\n",
    "    ):\n",
    "        axs[0][j].set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config and model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "config = parse_args(\"segmentation\", False, [])\n",
    "img2text = Img2Text(config)\n",
    "diffusion_dtype = torch.float16 if config.diffusion.dtype == \"fp16\" else torch.float32\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    config.diffusion.variant,\n",
    "    torch_dtype=diffusion_dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=config.model_dir,\n",
    "    device_map=config.diffusion.device_map,\n",
    ")\n",
    "# register attention processor for attention hooks\n",
    "pipe.unet.set_attn_processor(AttnProcessor())\n",
    "store_hook = AvgAttentionStoreHook(pipe)\n",
    "diffusion = Diffusion(pipe)\n",
    "text_emb_null = diffusion.encode_prompt(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "# voc_10_car img: 0\n",
    "img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2009_003722.jpg\"\n",
    "cls_names = [\"car\", \"chair\", \"motorbike\", \"people\"]\n",
    "# voc_10_car img: 2\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2011_001949.jpg\"\n",
    "# cls_names = [\"bicycle\", \"buses\", \"car\", \"people\"]\n",
    "# voc_10_car img: 3\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2010_001246.jpg\"\n",
    "# cls_names = [\"bicycle\", \"car\", \"people\", \"plant\"]\n",
    "# voc_10_car img: 4\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2008_005139.jpg\"\n",
    "# cls_names = [\"buses\", \"car\", \"motorbike\", \"plant\"]\n",
    "# voc_10_car img: 6\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2008_003978.jpg\"\n",
    "# cls_names = [\"car\", \"dog\", \"people\", \"plant\"]\n",
    "# voc_10_car img: 7\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2009_001997.jpg\"\n",
    "# cls_names = [\"bicycle\", \"buses\", \"car\", \"people\"]\n",
    "threshold = 0.6\n",
    "text_prompt = \"a photograph of {source_cls}\"  # text prompt to generate source_gen\n",
    "text_template = (\n",
    "    \"a photograph of ({source_cls})++ {source_gen}\"  # text template to generate mask\n",
    ")\n",
    "collect_timesteps = [[\"random\", 1, 200, 100]]\n",
    "\n",
    "# post process input\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "collect_timesteps = parse_timesteps(collect_timesteps)\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "w, h = img.size\n",
    "z_source = diffusion.encode_vae_image(img)\n",
    "# use same noise for all classes at the same timestep\n",
    "collect_noise = torch.randn(\n",
    "    len(collect_timesteps), *z_source.shape[1:], device=z_source.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mask Per Class Using Multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data1: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    for cls_name in cls_names:\n",
    "        loss, loss_img = 0, 0\n",
    "        cls_to_data1[cls_name] = ClassData()\n",
    "        # text input\n",
    "        source_cls = cls_name\n",
    "        source_gen = img2text(img, img_path, text_prompt.format(source_cls=source_cls))\n",
    "        source_text = text_template.format(source_cls=source_cls, source_gen=source_gen)\n",
    "        cls_to_data1[cls_name].text = source_text\n",
    "        text_emb_source = diffusion.encode_prompt(source_text)\n",
    "        # pos of source_cls in source_text\n",
    "        source_text_id = pipe.tokenizer.encode(source_text)\n",
    "        source_cls_id = pipe.tokenizer.encode(source_cls)[1:-1]\n",
    "        for start in range(len(source_text_id) - len(source_cls_id) + 1):\n",
    "            if source_text_id[start : start + len(source_cls_id)] == source_cls_id:\n",
    "                pos = [start + i for i in range(len(source_cls_id))]\n",
    "                break\n",
    "        if pos[-1] + 1 < len(source_text_id) and pipe.tokenizer.decode(\n",
    "            source_text_id[pos[-1] + 1]\n",
    "        ).endswith(\"ing\"):\n",
    "            pos.append(pos[-1] + 1)\n",
    "        # collect attention maps\n",
    "        store_hook.reset()\n",
    "        for idx, timestep in enumerate(collect_timesteps):\n",
    "            z_t_source, eps = diffusion.noise_input(\n",
    "                z_source, timestep, collect_noise[idx]\n",
    "            )\n",
    "            eps_pred = diffusion.get_eps_prediction(\n",
    "                [z_t_source],\n",
    "                [timestep],\n",
    "                [text_emb_source],\n",
    "                {\"attention_hooks\": [store_hook]},\n",
    "            )\n",
    "            loss += F.mse_loss(eps_pred, eps, reduction=\"mean\")\n",
    "            loss_img += torch.abs(eps_pred - eps).to(torch.float32)\n",
    "        # loss = sigma(mse(eps_pred,eps))\n",
    "        # loss_img = sigma(abs(eps_pred-eps))\n",
    "        cls_to_data1[cls_name].loss = loss\n",
    "        loss_img = loss_img.sum(dim=1).squeeze()\n",
    "        loss_img = loss_img.view(1, 1, loss_img.shape[0], loss_img.shape[1])\n",
    "        loss_img: T = F.interpolate(loss_img, size=(h, w), mode=\"bilinear\")\n",
    "        loss_img = (loss_img - loss_img.min()) / (loss_img.max() - loss_img.min()) * 255\n",
    "        cls_to_data1[cls_name].loss_img = (\n",
    "            loss_img.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # cross attention\n",
    "        mask = aggregate_cross_att(store_hook, 0, pos, config)\n",
    "        cls_to_data1[cls_name].cross_att = mask.clone()\n",
    "        max_res = round(sqrt(mask.shape[0]))\n",
    "        cross_att = mask.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min()) * 255\n",
    "        )\n",
    "        cls_to_data1[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # heatmap\n",
    "        self_att = aggregate_self_att(store_hook, 0, config)\n",
    "        cls_to_data1[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data1[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data1):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data1[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mask For All Classes In One Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data2: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    source_text = \",\".join(cls_names)\n",
    "    text_emb_source = diffusion.encode_prompt(source_text)\n",
    "    loss, loss_img = 0, 0\n",
    "    # collect attention maps\n",
    "    store_hook.reset()\n",
    "    for idx, timestep in enumerate(collect_timesteps):\n",
    "        z_t_source, eps = diffusion.noise_input(z_source, timestep, collect_noise[idx])\n",
    "        eps_pred = diffusion.get_eps_prediction(\n",
    "            [z_t_source],\n",
    "            [timestep],\n",
    "            [text_emb_source],\n",
    "            {\"attention_hooks\": [store_hook]},\n",
    "        )\n",
    "        loss += F.mse_loss(eps_pred, eps, reduction=\"mean\")\n",
    "        loss_img += torch.abs(eps_pred - eps).to(torch.float32)\n",
    "    # loss = sigma(mse(eps_pred,eps))\n",
    "    # loss_img = sigma(abs(eps_pred-eps))\n",
    "    loss_img = loss_img.sum(dim=1).squeeze()\n",
    "    loss_img = loss_img.view(1, 1, loss_img.shape[0], loss_img.shape[1])\n",
    "    loss_img: T = F.interpolate(loss_img, size=(h, w), mode=\"bilinear\")\n",
    "    loss_img = (loss_img - loss_img.min()) / (loss_img.max() - loss_img.min()) * 255\n",
    "    loss_img = loss_img.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "\n",
    "    for cls_name in cls_names:\n",
    "        cls_to_data2[cls_name] = ClassData()\n",
    "        cls_to_data2[cls_name].text = source_text\n",
    "        # pos of cls_name in source_text\n",
    "        source_text_id = pipe.tokenizer.encode(source_text)\n",
    "        source_cls_id = pipe.tokenizer.encode(cls_name)[1:-1]\n",
    "        for start in range(len(source_text_id) - len(source_cls_id) + 1):\n",
    "            if source_text_id[start : start + len(source_cls_id)] == source_cls_id:\n",
    "                pos = [start + i for i in range(len(source_cls_id))]\n",
    "                break\n",
    "        # loss data\n",
    "        cls_to_data2[cls_name].loss = loss\n",
    "        cls_to_data2[cls_name].loss_img = loss_img\n",
    "        # cross attention\n",
    "        mask = aggregate_cross_att(store_hook, 0, pos, config)\n",
    "        cls_to_data2[cls_name].cross_att = mask.clone()\n",
    "        max_res = round(sqrt(mask.shape[0]))\n",
    "        cross_att = mask.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min()) * 255\n",
    "        )\n",
    "        cls_to_data2[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # heatmap\n",
    "        self_att = aggregate_self_att(store_hook, 0, config)\n",
    "        cls_to_data2[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data2[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data2):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data2[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mask For \"Start\", \"Null Text\" And \"End\" Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data3: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    source_text = \"\"\n",
    "    special_cls_names = [\"start\", \"null\", \"end\"]\n",
    "    text_emb_source = diffusion.encode_prompt(source_text)\n",
    "    loss, loss_img = 0, 0\n",
    "    # collect attention maps\n",
    "    store_hook.reset()\n",
    "    for idx, timestep in enumerate(collect_timesteps):\n",
    "        z_t_source, eps = diffusion.noise_input(z_source, timestep, collect_noise[idx])\n",
    "        eps_pred = diffusion.get_eps_prediction(\n",
    "            [z_t_source],\n",
    "            [timestep],\n",
    "            [text_emb_source],\n",
    "            {\"attention_hooks\": [store_hook]},\n",
    "        )\n",
    "        loss += F.mse_loss(eps_pred, eps, reduction=\"mean\")\n",
    "        loss_img += torch.abs(eps_pred - eps).to(torch.float32)\n",
    "    # loss = sigma(mse(eps_pred,eps))\n",
    "    # loss_img = sigma(abs(eps_pred-eps))\n",
    "    loss_img = loss_img.sum(dim=1).squeeze()\n",
    "    loss_img = loss_img.view(1, 1, loss_img.shape[0], loss_img.shape[1])\n",
    "    loss_img: T = F.interpolate(loss_img, size=(h, w), mode=\"bilinear\")\n",
    "    loss_img = (loss_img - loss_img.min()) / (loss_img.max() - loss_img.min()) * 255\n",
    "    loss_img = loss_img.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "\n",
    "    for cls_name in special_cls_names:\n",
    "        cls_to_data3[cls_name] = ClassData()\n",
    "        cls_to_data3[cls_name].text = source_text\n",
    "        # pos of cls_name in source_text\n",
    "        if cls_name == \"start\":\n",
    "            pos = [0]\n",
    "        elif cls_name == \"null\":\n",
    "            pos = [i for i in range(1, text_emb_source.shape[0] - 1)]\n",
    "        else:\n",
    "            pos = [text_emb_source.shape[0] - 1]\n",
    "        # loss data\n",
    "        cls_to_data3[cls_name].loss = loss\n",
    "        cls_to_data3[cls_name].loss_img = loss_img\n",
    "        # cross attention\n",
    "        mask = aggregate_cross_att(store_hook, 0, pos, config)\n",
    "        cls_to_data3[cls_name].cross_att = mask.clone()\n",
    "        max_res = round(sqrt(mask.shape[0]))\n",
    "        cross_att = mask.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min()) * 255\n",
    "        )\n",
    "        cls_to_data3[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # heatmap\n",
    "        self_att = aggregate_self_att(store_hook, 0, config)\n",
    "        cls_to_data3[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data3[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data3):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data3[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Data And Generate The Final Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data4: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    for cls_name in cls_names:\n",
    "        cls_to_data4[cls_name] = ClassData()\n",
    "        # cross attention\n",
    "        # get cross attention mask from cls_to_data2\n",
    "        mask = cls_to_data2[cls_name].cross_att\n",
    "        cls_to_data4[cls_name].cross_att = mask.clone()\n",
    "        cls_to_data4[cls_name].cross_att_img = cls_to_data2[cls_name].cross_att_img\n",
    "        # heatmap\n",
    "        # get self attention mask from cls_to_data3 [START] token\n",
    "        self_att = cls_to_data3[\"start\"].self_att\n",
    "        cls_to_data4[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data4[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data4):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data4[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
