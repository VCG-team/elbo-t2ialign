{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is used to generate and visualize masks for a given image\n",
    "# input: image path, class names, threshold, text_prompt, text_template, timesteps\n",
    "# output: source img, loss, cross att, heatmap, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package and helper functions\n",
    "import random\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import AutoPipelineForText2Image, StableDiffusion3Pipeline\n",
    "from diffusers.models.attention_processor import Attention\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.attention_control import (\n",
    "    AttentionStoreHook,\n",
    "    AttnProcessor,\n",
    "    JointAttnProcessor,\n",
    "    aggregate_cross_att,\n",
    "    aggregate_self_att,\n",
    ")\n",
    "from utils.diffusion import Diffusion\n",
    "from utils.img2text import Img2Text\n",
    "from utils.parse_args import parse_args\n",
    "\n",
    "T = torch.Tensor\n",
    "TL = List[T]\n",
    "TN = Optional[T]\n",
    "\n",
    "\n",
    "# copied from run_segmentation.py\n",
    "class AvgAttentionStoreHook(AttentionStoreHook):\n",
    "    @torch.inference_mode\n",
    "    def forward(self, attn: Attention, q: T, k: T, v: T, sim: T, out: T) -> T:\n",
    "        \"\"\"\n",
    "        q: (b, h, i, d)\n",
    "        k, v: (b, h, j, d)\n",
    "        sim: (b, h, i, j)\n",
    "        out: (b, i, n) | ((b, i, n), (b, j, m))\n",
    "        \"\"\"\n",
    "        sim_mean = sim.mean(dim=1)\n",
    "        # sd3 like MMDiT attention will return a tuple\n",
    "        if isinstance(out, tuple):\n",
    "            img_len = out[0].shape[1]\n",
    "            txt_len = out[1].shape[1]\n",
    "            self.step_store[\"cross_att\"].append(sim_mean[:, :img_len, -txt_len:])\n",
    "            self.step_store[\"self_att\"].append(sim_mean[:, :img_len, :img_len])\n",
    "        else:\n",
    "            key = \"cross_att\" if attn.is_cross_attention else \"self_att\"\n",
    "            self.step_store[key].append(sim_mean)\n",
    "        return out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassData:\n",
    "    cross_att_img: Optional[np.ndarray] = None\n",
    "    heatmap: Optional[np.ndarray] = None\n",
    "    mask: Optional[np.ndarray] = None\n",
    "    text: str = \"\"\n",
    "    loss: TN = None\n",
    "    cross_att: TN = None\n",
    "    self_att: TN = None\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        setattr(self, key, value)\n",
    "\n",
    "\n",
    "def parse_timesteps(timesteps: List[List]) -> List:\n",
    "    parsed_timesteps = []\n",
    "    for t in timesteps:\n",
    "        if len(t) == 3:\n",
    "            parsed_timesteps.extend(range(t[0], t[1], t[2]))\n",
    "        else:  # len(t) == 4, random sample\n",
    "            parsed_timesteps.extend([random.randint(t[1], t[2]) for _ in range(t[3])])\n",
    "    return parsed_timesteps\n",
    "\n",
    "\n",
    "def show_data(img: Image, cls_to_data: Dict[str, ClassData]):\n",
    "    for cls in cls_to_data:\n",
    "        print(\n",
    "            f\"class: {cls} --- loss: {cls_to_data[cls].loss} --- text: {cls_to_data[cls].text}\"\n",
    "        )\n",
    "\n",
    "    row_cnt = len(cls_to_data)\n",
    "    col_cnt = 4\n",
    "    fig, axs = plt.subplots(row_cnt, col_cnt, figsize=(col_cnt * 4, row_cnt * 4))\n",
    "    placeholder = np.zeros((img.size[1], img.size[0]), dtype=np.uint8)\n",
    "    placeholder_title = \"No Image Available\"\n",
    "\n",
    "    if row_cnt == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, (cls, data) in enumerate(cls_to_data.items()):\n",
    "        axs[i][0].set_ylabel(cls)\n",
    "        axs[i][0].imshow(img)\n",
    "        images = [data.cross_att_img, data.heatmap, data.mask]\n",
    "        for j, image in enumerate(images):\n",
    "            if image is not None:\n",
    "                axs[i][j + 1].imshow(image)\n",
    "            else:\n",
    "                axs[i][j + 1].imshow(placeholder)\n",
    "                axs[i][j + 1].set_title(placeholder_title)\n",
    "\n",
    "    for i in range(row_cnt):\n",
    "        for j in range(col_cnt):\n",
    "            axs[i][j].tick_params(\n",
    "                left=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False,\n",
    "                bottom=False,\n",
    "            )\n",
    "\n",
    "    for j, title in enumerate([\"source img\", \"cross att img\", \"heatmap\", \"mask\"]):\n",
    "        axs[0][j].set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config and model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "config = parse_args(\n",
    "    \"run_segmentation\",\n",
    "    False,\n",
    "    [\n",
    "        \"diffusion.variant=stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "        \"cross_gaussian_var=3\",\n",
    "        \"self_weight=[[1],[1]]\",\n",
    "    ],\n",
    ")\n",
    "img2text = Img2Text(config)\n",
    "diffusion_dtype = torch.float16 if config.diffusion.dtype == \"fp16\" else torch.float32\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    config.diffusion.variant,\n",
    "    torch_dtype=diffusion_dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=config.model_dir,\n",
    "    device_map=config.diffusion.device_map,\n",
    ")\n",
    "# register attention processor for attention hooks\n",
    "if isinstance(pipe, StableDiffusion3Pipeline):\n",
    "    pipe.transformer.set_attn_processor(JointAttnProcessor())\n",
    "else:\n",
    "    pipe.unet.set_attn_processor(AttnProcessor())\n",
    "store_hook = AvgAttentionStoreHook(pipe)\n",
    "diffusion = Diffusion(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "# voc_10_car img: 0\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2009_003722.jpg\"\n",
    "# cls_names = [\"car\", \"chair\", \"motorbike\", \"people\"]\n",
    "# voc_10_car img: 2\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2011_001949.jpg\"\n",
    "# cls_names = [\"bicycle\", \"buses\", \"car\", \"people\"]\n",
    "# voc_10_car img: 3\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2010_001246.jpg\"\n",
    "# cls_names = [\"bicycle\", \"car\", \"people\", \"plant\"]\n",
    "# voc_10_car img: 4\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2008_005139.jpg\"\n",
    "# cls_names = [\"buses\", \"car\", \"motorbike\", \"people\"]\n",
    "# voc_10_car img: 6\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2008_003978.jpg\"\n",
    "# cls_names = [\"car\", \"dog\", \"people\", \"plant\"]\n",
    "# voc_10_car img: 7\n",
    "# img_path = \"./datasets/VOCdevkit/VOC2012/JPEGImages/2009_001997.jpg\"\n",
    "# cls_names = [\"bicycle\", \"buses\", \"car\", \"people\"]\n",
    "# coco_10_car img: 0\n",
    "# img_path = \"./datasets/coco_stuff164k/images/val2017/000000006040.jpg\"\n",
    "# cls_names = [\"person\", \"car\", \"train\", \"truck\"]\n",
    "# coco_10_car img: 1\n",
    "# img_path = \"./datasets/coco_stuff164k/images/val2017/000000007088.jpg\"\n",
    "# cls_names = [\"person\", \"car\", \"truck\", \"umbrella\"]\n",
    "# coco_10_car img: 2\n",
    "# img_path = \"./datasets/coco_stuff164k/images/val2017/000000009891.jpg\"\n",
    "# cls_names = [\"person\", \"car\", \"backpack\", \"necktie\", \"suitcase\"]\n",
    "# coco_10_car img: 3\n",
    "img_path = \"./datasets/coco_stuff164k/images/val2017/000000061108.jpg\"\n",
    "cls_names = [\"bicycle\", \"car\", \"bench\", \"dog\"]\n",
    "# coco_10_car img: 5\n",
    "# img_path = \"./datasets/coco_stuff164k/images/val2017/000000124798.jpg\"\n",
    "# cls_names = [\"person\", \"car\", \"bus\", \"truck\", \"umbrella\"]\n",
    "# coco_10_car img: 6\n",
    "# img_path = \"./datasets/coco_stuff164k/images/val2017/000000125778.jpg\"\n",
    "# cls_names = [\"car\", \"bottle\", \"chair\", \"sofa\", \"potted plant\", \"vase\"]\n",
    "# coco_10_car img: 7\n",
    "# img_path = \"./datasets/coco_stuff164k/images/val2017/000000127092.jpg\"\n",
    "# cls_names = [\"person\", \"car\", \"fire hydrant\", \"stop sign\", \"handbag\", \"potted plant\"]\n",
    "# img_path = \"./datasets/elbo_toy/compose/count/wood.png\"\n",
    "# cls_names = [\"wooden cat\", \"vase\", \"teapot\"]\n",
    "threshold = 0.6\n",
    "text_prompt = \"a photo of {source_cls}\"  # text prompt to generate source_gen\n",
    "text_template = \"a photo of {source_cls}\"  # text template to generate mask\n",
    "collect_timesteps = [[20, 201, 20]]\n",
    "loss_timesteps = [[200, 800, 29]]\n",
    "\n",
    "# post process input\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "collect_timesteps = parse_timesteps(collect_timesteps)\n",
    "loss_timesteps = parse_timesteps(loss_timesteps)\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "w, h = img.size\n",
    "z_source = diffusion.encode_vae_image(img)\n",
    "# use same noise for all classes at the same timestep\n",
    "collect_noise = torch.randn(\n",
    "    len(collect_timesteps), *z_source.shape[1:], device=z_source.device\n",
    ")\n",
    "loss_noise = torch.randn(\n",
    "    len(loss_timesteps), *z_source.shape[1:], device=z_source.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mask Per Class Using Multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data1: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    for cls_name in cls_names:\n",
    "        loss = 0\n",
    "        cls_to_data1[cls_name] = ClassData()\n",
    "        # text input\n",
    "        source_cls = cls_name\n",
    "        source_gen = img2text(img, img_path, text_prompt.format(source_cls=source_cls))\n",
    "        no_other_cls = \"\"\n",
    "        for cls in cls_names:\n",
    "            if cls != source_cls:\n",
    "                no_other_cls += f\", without {cls}\"\n",
    "        source_text = text_template.format(\n",
    "            source_cls=source_cls, source_gen=source_gen, no_other_cls=no_other_cls\n",
    "        )\n",
    "        cls_to_data1[cls_name].text = source_text\n",
    "        text_emb_source = diffusion.encode_prompt(source_text)\n",
    "        # pos of source_cls in source_text\n",
    "        source_text_id = pipe.tokenizer.encode(source_text)\n",
    "        source_cls_id = pipe.tokenizer.encode(source_cls)[1:-1]\n",
    "        for start in range(len(source_text_id) - len(source_cls_id) + 1):\n",
    "            if source_text_id[start : start + len(source_cls_id)] == source_cls_id:\n",
    "                pos = [start + i for i in range(len(source_cls_id))]\n",
    "                break\n",
    "        if pos[-1] + 1 < len(source_text_id) and pipe.tokenizer.decode(\n",
    "            source_text_id[pos[-1] + 1]\n",
    "        ).endswith(\"ing\"):\n",
    "            pos.append(pos[-1] + 1)\n",
    "        # collect attention maps\n",
    "        store_hook.reset()\n",
    "        for idx, timestep in enumerate(loss_timesteps):\n",
    "            loss += diffusion.get_elbo(\n",
    "                z=z_source,\n",
    "                text_emb=text_emb_source,\n",
    "                timestep=timestep,\n",
    "                eps=loss_noise[idx],\n",
    "                cross_attention_kwargs={\"attention_hooks\": [store_hook]},\n",
    "            )\n",
    "        cls_to_data1[cls_name].loss = loss\n",
    "        # cross attention\n",
    "        mask = aggregate_cross_att(store_hook, 0, pos, config)\n",
    "        cls_to_data1[cls_name].cross_att = mask.clone()\n",
    "        max_res = round(sqrt(mask.shape[0]))\n",
    "        cross_att = mask.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min()) * 255\n",
    "        )\n",
    "        cls_to_data1[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # heatmap\n",
    "        self_att = aggregate_self_att(store_hook, 0, config)\n",
    "        cls_to_data1[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data1[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data1):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data1[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mask For All Classes In One Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data2: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    source_text = \"a photo of \" + \", \".join(cls_names)\n",
    "    text_emb_source = diffusion.encode_prompt(source_text)\n",
    "    loss = 0\n",
    "    # collect attention maps\n",
    "    store_hook.reset()\n",
    "    for idx, timestep in enumerate(collect_timesteps):\n",
    "        loss += diffusion.get_elbo(\n",
    "            z=z_source,\n",
    "            text_emb=text_emb_source,\n",
    "            timestep=timestep,\n",
    "            eps=collect_noise[idx],\n",
    "            cross_attention_kwargs={\"attention_hooks\": [store_hook]},\n",
    "        )\n",
    "    for cls_name in cls_names:\n",
    "        cls_to_data2[cls_name] = ClassData()\n",
    "        cls_to_data2[cls_name].text = source_text\n",
    "        # pos of cls_name in source_text\n",
    "        source_text_id = pipe.tokenizer.encode(source_text)\n",
    "        source_cls_id = pipe.tokenizer.encode(cls_name)[1:-1]\n",
    "        for start in range(len(source_text_id) - len(source_cls_id) + 1):\n",
    "            if source_text_id[start : start + len(source_cls_id)] == source_cls_id:\n",
    "                pos = [start + i for i in range(len(source_cls_id))]\n",
    "                break\n",
    "        # loss data\n",
    "        cls_to_data2[cls_name].loss = loss\n",
    "        # cross attention\n",
    "        mask = aggregate_cross_att(store_hook, 0, pos, config)\n",
    "        cls_to_data2[cls_name].cross_att = mask.clone()\n",
    "        max_res = round(sqrt(mask.shape[0]))\n",
    "        cross_att = mask.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min()) * 255\n",
    "        )\n",
    "        cls_to_data2[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # heatmap\n",
    "        self_att = aggregate_self_att(store_hook, 0, config)\n",
    "        cls_to_data2[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data2[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data2):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data2[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mask For \"Start\", \"Null Text\" And \"End\" Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data3: Dict[str, ClassData] = {}\n",
    "    heatmaps = []\n",
    "    source_text = \"\"\n",
    "    special_cls_names = [\"start\", \"null\", \"end\"]\n",
    "    text_emb_source = diffusion.encode_prompt(source_text)\n",
    "    text_emb_len = (\n",
    "        text_emb_source[0].shape[0]\n",
    "        if isinstance(text_emb_source, tuple)\n",
    "        else text_emb_source.shape[0]\n",
    "    )\n",
    "    loss = 0\n",
    "    # collect attention maps\n",
    "    store_hook.reset()\n",
    "    for idx, timestep in enumerate(collect_timesteps):\n",
    "        loss += diffusion.get_elbo(\n",
    "            z=z_source,\n",
    "            text_emb=text_emb_source,\n",
    "            timestep=timestep,\n",
    "            eps=collect_noise[idx],\n",
    "            cross_attention_kwargs={\"attention_hooks\": [store_hook]},\n",
    "        )\n",
    "    for cls_name in special_cls_names:\n",
    "        cls_to_data3[cls_name] = ClassData()\n",
    "        cls_to_data3[cls_name].text = source_text\n",
    "        # pos of cls_name in source_text\n",
    "        if cls_name == \"start\":\n",
    "            pos = [0]\n",
    "        elif cls_name == \"null\":\n",
    "            pos = [i for i in range(1, text_emb_len - 1)]\n",
    "        else:\n",
    "            pos = [text_emb_len - 1]\n",
    "        # loss data\n",
    "        cls_to_data3[cls_name].loss = loss\n",
    "        # cross attention\n",
    "        mask = aggregate_cross_att(store_hook, 0, pos, config)\n",
    "        cls_to_data3[cls_name].cross_att = mask.clone()\n",
    "        max_res = round(sqrt(mask.shape[0]))\n",
    "        cross_att = mask.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min()) * 255\n",
    "        )\n",
    "        cls_to_data3[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        # heatmap\n",
    "        self_att = aggregate_self_att(store_hook, 0, config)\n",
    "        cls_to_data3[cls_name].self_att = self_att.clone()\n",
    "        mask = torch.matmul(self_att, mask)\n",
    "        mask = mask.view(1, 1, max_res, max_res)\n",
    "        mask: T = F.interpolate(mask, size=(h, w), mode=\"bilinear\")\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        heatmaps.append(mask.squeeze())\n",
    "        cls_to_data3[cls_name].heatmap = (\n",
    "            mask.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # mask\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold * 255] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data3):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data3[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Data And Generate The Final Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    cls_to_data4: Dict[str, ClassData] = {}\n",
    "    losses = []\n",
    "    # collect cross attention and ELBO(losses)\n",
    "    for cls_name in cls_names:\n",
    "        cls_to_data4[cls_name] = ClassData()\n",
    "        cls_to_data4[cls_name].cross_att = cls_to_data2[cls_name].cross_att.clone()\n",
    "        cls_to_data4[cls_name].loss = cls_to_data1[cls_name].loss\n",
    "        cls_to_data4[cls_name].text = cls_to_data1[cls_name].text\n",
    "        losses.append(cls_to_data1[cls_name].loss)\n",
    "    losses = torch.stack(losses, dim=0).squeeze()\n",
    "    # prepare normalization Temperature using ELBO\n",
    "    loss_mean = losses.mean()\n",
    "    loss_std = losses.std()\n",
    "    loss_minus_mean = losses - loss_mean\n",
    "    loss_min_max = (losses - losses.min()) / (losses.max() - losses.min())\n",
    "    temperature = torch.pow(3, loss_min_max)\n",
    "    # cross attention normalization with ELBO\n",
    "    for idx, cls_name in enumerate(cls_names):\n",
    "        cross_att = cls_to_data4[cls_name].cross_att\n",
    "        cross_att = (\n",
    "            (cross_att - cross_att.min()) / (cross_att.max() - cross_att.min())\n",
    "        ) ** temperature[idx]\n",
    "        cls_to_data4[cls_name].cross_att = cross_att\n",
    "    # get cross attention image after normalization\n",
    "    for cls_name in cls_names:\n",
    "        max_res = round(sqrt(cls_to_data4[cls_name].cross_att.shape[0]))\n",
    "        cross_att = cls_to_data4[cls_name].cross_att.view(1, 1, max_res, max_res)\n",
    "        cross_att: T = F.interpolate(cross_att, size=(h, w), mode=\"bilinear\")\n",
    "        cross_att = cross_att.clamp(0, 1) * 255\n",
    "        cls_to_data4[cls_name].cross_att_img = (\n",
    "            cross_att.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "    # using self attention to refine cross attention\n",
    "    heatmaps = []\n",
    "    for cls_name in cls_names:\n",
    "        cls_to_data4[cls_name].self_att = cls_to_data2[cls_name].self_att.clone()\n",
    "        heatmap = torch.matmul(\n",
    "            cls_to_data4[cls_name].self_att, cls_to_data4[cls_name].cross_att\n",
    "        )\n",
    "        heatmap = heatmap.view(1, 1, max_res, max_res)\n",
    "        heatmap: T = F.interpolate(heatmap, size=(h, w), mode=\"bilinear\")\n",
    "        # min-max normalization of heatmaps\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "        cls_to_data4[cls_name].heatmap = (\n",
    "            (heatmap * 255).squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        heatmaps.append(heatmap.squeeze())\n",
    "    # let heatmap of each class mutually exclusive\n",
    "    heatmaps = torch.stack(heatmaps, dim=0)\n",
    "    heatmaps_argmax = heatmaps.argmax(dim=0)\n",
    "    for cls_idx in range(len(cls_names)):\n",
    "        heatmaps[cls_idx][heatmaps_argmax != cls_idx] = 0\n",
    "    # generate mask using heatmaps\n",
    "    heatmaps_idx = heatmaps.argmax(dim=0)\n",
    "    heatmaps_val = heatmaps.max(dim=0).values\n",
    "    heatmaps_idx[heatmaps_val < threshold] = 255  # background\n",
    "    for idx, cls_name in enumerate(cls_to_data4):\n",
    "        mask = (heatmaps_idx == idx).float() * 255\n",
    "        cls_to_data4[cls_name].mask = mask.cpu().detach().numpy().astype(np.uint8)\n",
    "    # show data\n",
    "    show_data(img, cls_to_data4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elbo-t2ialign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
