# ELBO-T2IAlign

## Setup

### Environment

1. Clone repository and install git lfs.
```bash
git clone https://github.com/VCG-team/elbo-t2ialign
cd elbo-t2ialign
git lfs install
git lfs pull
```

2. Create [conda](https://conda.io/) env with `environment.yaml`.
```bash
conda env create -f environment.yaml
conda activate elbo-t2ialign
# download spacy model for part-of-speech tags
python -m spacy download en_core_web_sm
```

### Data Preparation

1. We mainly used [mmsegmentation](https://github.com/open-mmlab/mmsegmentation) to prepare the data. Specifically, we followed this [guideline](https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md) to prepare PASCAL VOC (including VOCaug), PASCAL Context and COCO Stuff 164k datasets.

2. After preparing these datasets, please link them to the project folder. The overall file structure is as follows(all the datasets are linked to `./datasets`):
    ```
    elbo-t2ialign
    ├── configs
    ├── data
    ├── utils
    ├── README.md
    ├── ...
    ├── datasets
    │   ├── VOCdevkit
    │   │   ├── VOC2012
    │   │   │   ├── JPEGImages
    │   │   │   ├── SegmentationClassAug
    │   │   │   ├── ImageSets
    │   │   │   │   ├── Segmentation
    │   │   ├── VOC2010
    │   │   │   ├── JPEGImages
    │   │   │   ├── SegmentationClassContext
    │   │   │   ├── ImageSets
    │   │   │   │   ├── SegmentationContext
    │   │   │   │   │   ├── train.txt
    │   │   │   │   │   ├── val.txt
    │   │   ├── VOCaug
    │   │   │   ├── dataset
    │   ├── coco_stuff164k
    │   │   ├── images
    │   │   │   ├── train2017
    │   │   │   ├── val2017
    │   │   ├── annotations
    │   │   │   ├── train2017
    │   │   │   ├── val2017
    │   ├── voc_sim
    │   │   ├── images
    │   │   ├── annotations
    │   ├── coco_cap
    │   │   ├── images
    │   │   ├── annotations
    │   ├── ade
    │   │   ├── ADEChallengeData2016
    │   │   |   ├── images
    │   │   |   ├── annotations
    ```

3. Finally, download [SBD dataset](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126343&casa_token=cOQGLW2KWqUAAAAA:Z-QHpQPf8Pnb07A75yBm2muYjqJwYUYPFbwwxMFHRcjRX0zl45kEGNqyTEPH7irB2QbabZbn&tag=1) annotations via this [link](https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=0). After downloading, replace `./datasets/VOCdevkit/VOC2012/SegmentationClassAug` with the downloaded folder. This step makes sure the evaluation is consistent with [MCTFormer](https://github.com/xulianuwa/MCTformer).

> Note:
> 1. `./data/voc/cls_labels.npy` is copied from [MCTFormer](https://github.com/xulianuwa/MCTformer).
> 2. `./data/context/cls_labels.npy` and `./data/coco/cls_labels.npy` are generated by unreleased script(have been carefully checked).
> 3. `**/cls_labels.npy` have the same format(have been carefully checked).
> 4. `./data/*/val_id.txt` all keep the same with validation set of original datasets(have been carefully checked).
> 5. If you have trouble downloading VOCaug dataset, please refer to [this PR](https://github.com/open-mmlab/mmsegmentation/pull/3654) to mmsegmentation.
> 6. The SBD dataset download link is from [MCTFormer](https://github.com/xulianuwa/MCTformer) README.

## Usage

Reproducing the results will need about 10G GPU memory. We are still working on reducing the memory cost for reproductions.

To test different settings, you can change configuration files in `config` folder, or pass command line arguments following [OmegaConf](https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#usage).

We provide a experiment script template `./scripts/experiment_template.sh` to run experiments. You can modify the script to run different experiments.

Welcome to open an issue if you have any questions. 

## Credits

We appreciate all open source projects that we use in this project:

- [mmsegmentation](https://github.com/open-mmlab/mmsegmentation), [diffusers](https://github.com/huggingface/diffusers), [transformers](https://github.com/huggingface/transformers)
- [MCTFormer](https://github.com/xulianuwa/MCTformer), [prompt-to-prompt](https://github.com/google/prompt-to-prompt), [clip-es](https://github.com/linyq2117/CLIP-ES)
- ...

## Citation
```bibtex

```
