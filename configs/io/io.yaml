# output config
output_path:
  coco: "./output/coco"
  context: "./output/context"
  voc: "./output/voc"
  voc_sim: "./output/voc_sim"
  coco_cap: "./output/coco_cap"
  ade20k: "./output/ade20k"

# input config
# fix random seed
seed: 4307
# whether use predicted classes to construct dataset
# if True, use cls_predict.npy(generated by classification.py) to construct dataset
# if False, use ground truth class labels to construct dataset(./data/{dataset_name}/cls_labels.npy)
use_cls_predict: False
# path to elbo_min_max.json file.
# if not None, use elbo_min_max.json to normalize cross attention
# if None, recompute elbo or omit elbo depending on segmentation method
elbo_path: ""

# cache path to save generated text
cache_dir: "./tmp/cache"
# cache path to save models
model_dir: "./models"

diffusion:
  variant: "runwayml/stable-diffusion-v1-5"
  device_map: "balanced"
  dtype: "fp16"
  use_safetensors: True
clip:
  variant: "openai/clip-vit-large-patch14"
  device_map: "balanced"
  dtype: "fp32"
img2text:
  variant: "Salesforce/blip-image-captioning-large"
  # some img2text models still don't support balanced device_map
  # see: https://github.com/huggingface/transformers/issues/29786
  device_map: "cuda:0"
  dtype: "fp32"
  # if api_url is not None, use api to convert image to text
  api_key: null
  api_url: null
