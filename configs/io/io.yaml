# output config
output_path:
  coco: "./output/coco"
  context: "./output/context"
  voc: "./output/voc"
  voc_sim: "./output/voc_sim"
  coco_cap: "./output/coco_cap"

# input config
# fix random seed
seed: 4307
# whether use predicted classes to construct dataset
# if True, use cls_predict.npy(generated by classification.py) to construct dataset
# if False, use ground truth class labels to construct dataset(./data/coco/cls_labels.npy etc.)
use_cls_predict: False
# path to cls_predict.npy, only used when use_cls_predict is True
# if leave empty, use default path: output_path/cls_predict.npy
cls_predict: ""

# cache path to save generated text
cache_dir: "./tmp/cache"
# cache path to save models
model_dir: "./models"

diffusion:
  variant: "runwayml/stable-diffusion-v1-5"
  device_map: "balanced"
  dtype: "fp16"
clip:
  variant: "openai/clip-vit-large-patch14"
  device_map: "balanced"
  dtype: "fp32"
img2text:
  variant: "Salesforce/blip-image-captioning-large"
  # some img2text models still don't support balanced device_map
  # see: https://github.com/huggingface/transformers/issues/29786
  device_map: "cuda:0"
  dtype: "fp32"
  # if api_url is not None, use api to convert image to text
  api_key: null
  api_url: null
