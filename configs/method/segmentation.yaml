# segmentation process settings (segmentation.py)
source_text:
  prompt: "a photograph of {source_cls}"
  template: "a photograph of {source_cls} {source_gen} and {bg_text}"
  compel_template: "a photograph of ({source_cls})++ {source_gen} and {bg_text}"
target_text:
  prompt: "a photograph of {source_cls}"
  template: "a photograph of {target_cls} {source_gen} and {bg_text}"
  compel_template: "a photograph of {target_cls} {source_gen} and {bg_text}"
target_cls_strategy: "special_token" # "special_token" or "synonym"
special_token: "''"
# see ./configs/io/io.yaml for details
# by io.yaml default, use_cls_prediction is False
# for open vocabulary segmentation, we need to use predicted classes to construct dataset
use_cls_predict: True
optimize_timesteps: # timesteps to optimize image, format: [start, end, step]
  - [1, 2, 1]
  - [25, 151, 25]
delay_collection: False # whether to delay the collection of attention maps
collect_timesteps: # timesteps to collect attention maps, only valid when delay_collection is True, format: [start, end, step]
  - [1, 2, 1]
  - [25, 151, 25]
loss_type: "dds" # loss function, "dds" or "sds" or "cds" or "none"
enable_mask: False # whether enable mask for loss calculation
mask_threshold: 0.3 # mask threshold for loss calculation
lr: 1.0e-1 # learning rate
guidance_scale: null # null or float, if null, use the default value of pipeline
save_img: False # whether to save optimized images(decoded z_target)

# attention control settings (attention_control.py)
# diffusion models use unet/dit with cross/self attentions to predict noise
# as image pass unet/dit block, image and attention resolution change accordingly
# only part of attn layers have strong semantic information(https://arxiv.org/abs/2309.04109)
# so we can choose some layers to achieve better performance
cross_gaussian_var: 0 # variance of gaussian distribution for cross attention
cross_weight: [1, 1, 10, 15, 10, 1, 1] # enabled when cross_gaussian_var == 0, weight of cross attention for each resolution
self_gaussian_var: [] # variance of gaussian distribution for self attention
self_weight: # enabled when self_gaussian_var == [], weight of self attention for each resolution
  - [1, 0, 0, 0, 0, 0, 1]
  - [0, 0, 0, 0, 0, 0, 1]
merge_type: "latent" # merge 'latent' or 'attention' of target and source image
target_factor: 0.5 # ratio of target merged into source, result = source + target_factor * (source - target)

# loss settings (loss.py)
# dds & sds loss hyperparameters
dds_loss_weight: 2000.0 # coefficient of dds loss
alpha_exp: 0.0
sigma_exp: 0.0
# cds loss hyperparameters
cut_loss_weight: 3.0 # coefficient of cut loss
patch_size: [1, 2]
n_patches: 256
