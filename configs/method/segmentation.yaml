# segmentation process settings (segmentation_with_dds.py/segmentation_with_elbo.py)
source_text:
  prompt: "a photo of {source_cls}"
  template: "a photo of {source_cls} {source_gen} and {bg_text}"
  compel_template: "a photo of ({source_cls})++ {source_gen} and {bg_text}"
target_text:
  prompt: "a photo of {source_cls}"
  template: "a photo of {target_cls} {source_gen} and {bg_text}"
  compel_template: "a photo of {target_cls} {source_gen} and {bg_text}"
elbo_text:
  prompt: "a photo of {source_cls}"
  template: "a photo of {source_cls}"
  compel_template: "a photo of {source_cls}"
target_cls_strategy: "special_token" # "special_token" or "synonym"
special_token: "''"
# see ./configs/io/io.yaml for details
# by io.yaml default, use_cls_prediction is False
# for open vocabulary segmentation, we need to use predicted classes to construct dataset
use_cls_predict: False
# timesteps to optimize image, recommend using random timesteps as DDS(ICCV 2023)
# format: [start, end(exclude), step] or ["random", start, end(include), num]
optimize_timesteps:
  - ["random", 50, 949, 7]
# timestpes to get elbo prediction, refer to Diffusion Classifier(ICCV 2023)
# format: [start, end(exclude), step] or ["random", start, end(include), num]
elbo_timesteps:
  - [1, 999, 50]
# whether to use ddim inversion to optimize image
ddim_inversion: False
# timesteps to collect attention maps, default to align with ddim inversion default step
# format: [start, end(exclude), step] or ["random", start, end(include), num]
collect_timesteps:
  - [20, 201, 20]
loss_type: "dds" # loss function, "dds" or "sds" or "cds" or "none"
lr: 1.0e-1 # learning rate
elbo_strength: 5.0 # strength of using elbo to normalize cross attention
fix_temperature: False # whether to fix temperature of normalizing cross attention
guidance_scale: null # null or float, if null, use the default value of pipeline
save_img: False # whether to save optimized images(decoded z_target)
save_cross_att: False # whether to save cross attention maps
save_elbo: False # whether to save relative elbo

# attention control settings (attention_control.py)
# diffusion models use unet/dit with cross/self attentions to predict noise
# as image pass unet/dit block, image and attention resolution change accordingly
# only part of attn layers have strong semantic information(https://arxiv.org/abs/2309.04109)
# so we can choose some layers to achieve better performance
cross_gaussian_var: 0 # variance of gaussian distribution for cross attention
cross_weight: [1, 1, 10, 15, 10, 1, 1] # enabled when cross_gaussian_var == 0, weight of cross attention for each resolution
self_weight: # weight of self attention for each resolution
  - [1, 0, 0, 0, 0, 0, 1]
  - [0, 0, 0, 0, 0, 0, 1]
merge_type: "latent" # merge 'latent' or 'attention' of target and source image
target_factor: 0.5 # ratio of target merged into source, result = source + target_factor * (source - target)

# loss settings (loss.py)
# dds & sds loss hyperparameters
dds_loss_weight: 2000.0 # coefficient of dds loss
alpha_exp: 0.0
sigma_exp: 0.0
# cds loss hyperparameters
cut_loss_weight: 3.0 # coefficient of cut loss
patch_size: [1, 2]
n_patches: 256
